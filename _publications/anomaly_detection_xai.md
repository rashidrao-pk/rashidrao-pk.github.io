---
title: "Can I trust my anomaly detection system? A case study based on explainable AI"
collection: publications
category: conferences
permalink: /publication/anomaly_detection_xai
excerpt: 'A Case study to highlight use of VAE-GAN based Gen-AI approach to detect Anomalies in Industrial Inspection systems.'
date: 17-07-2024
venue: 'xAI 2024 | Explainable Artificial Intelligence'
location: "La Valletta, Malta"
slidesurl: 'http://rashidrao-pk.github.io/files/anomaly_detection_xai_w_slides.pdf'
paperurl: 'http://rashidrao-pk.github.io/files/anomaly_detection_xai_w_paper.pdf'
citation: 'Rashid,Muhammad et al. (2024). &quot;.&quot; <i>In World Conference on Explainable Artificial Intelligence, pp. 243-254. Cham: Springer Nature Switzerland, 2024</i>.'
---

Generative models based on variational autoencoders are a popular technique for detecting anomalies in images in a semi-supervised context. A common approach employs the anomaly score to detect the presence of anomalies, and it is known to reach high level of accuracy on benchmark datasets. However, since anomaly scores are computed from reconstruction disparities, they often obscure the detection of various spurious features, raising concerns regarding their actual efficacy. 
This case study explores the robustness of an anomaly detection system based on variational autoencoder generative models through the use of eXplainable AI methods. The goal is to get a different perspective on the real performances of anomaly detectors that use reconstruction differences. In our case study we discovered that, in many cases, samples are detected as anomalous for the wrong or misleading factors. The method is available under: https://github.com/rashidrao-pk/anomaly_detection_trust_case_study

Special Session: Explainable AI for improved human-computer interaction
Conference: xAI 2024 | Explainable Artificial Intelligence
Link to talk:   https://xaiworldconference.com/2024/timetable/event/s-17-a-1/
